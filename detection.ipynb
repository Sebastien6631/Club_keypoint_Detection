{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Detection d'object de jonglerie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import optuna\n",
    "from torchvision.transforms import Normalize\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/tree/main/references/detection\n",
    "from references import transforms, utils, engine, train\n",
    "from references.utils import collate_fn\n",
    "from references.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation des données de Deeplabcut au format COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_path = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_youtube-sebastien-2024-02-05\\labeled-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = []\n",
    "for item in os.listdir(labeled_data_path):\n",
    "   \n",
    "    folder_path = os.path.join(labeled_data_path, item)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        if not item.endswith(\"_labeled\"):\n",
    "\n",
    "            for idx,it in enumerate(os.listdir(folder_path)):\n",
    "\n",
    "                if it.endswith(\"csv\"):\n",
    "\n",
    "                    csv_path.append(os.path.join(folder_path, it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1 = pd.read_csv(csv_path[0])\n",
    "csv2 = pd.read_csv(csv_path[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1 = csv1.drop(csv1.index[:3])\n",
    "csv2 = csv2.drop(csv2.index[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change le nom des frame pour simplifier l'addition de frames depuis différent dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(dataframe, dossier_images_source, dossier_images_destination,num_csv, idx):\n",
    "    \n",
    "    if not os.path.exists(dossier_images_destination):\n",
    "        os.makedirs(dossier_images_destination)\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        #if index < 3:\n",
    "        #    continue\n",
    "\n",
    "        nom_fichier = row['Unnamed: 2']\n",
    "        video_origin = row['Unnamed: 1']\n",
    "\n",
    "        if pd.isna(nom_fichier):\n",
    "            continue\n",
    "\n",
    "        chemin_source = os.path.join(dossier_images_source, nom_fichier)\n",
    "\n",
    "        nouveau_nom_fichier = f\"{video_origin}_{index-3 + idx}.png\"\n",
    "\n",
    "        chemin_destination = os.path.join(dossier_images_destination, nouveau_nom_fichier)\n",
    "\n",
    "        shutil.copy(chemin_source, chemin_destination)\n",
    "        \n",
    "        df_copie = dataframe.copy()\n",
    "\n",
    "        dataframe.at[index, 'Unnamed: 2'] = nouveau_nom_fichier\n",
    "\n",
    "    idx = index + idx\n",
    "    #idx = index-3 + idx\n",
    "\n",
    "    dataframe.to_csv(os.path.join(dossier_images_destination, f\"annotations_{num_csv}.csv\"), index=False)\n",
    "\n",
    "    return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossier_images_destination = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\\images\"\n",
    "idx=0\n",
    "\n",
    "for item in os.listdir(labeled_data_path):\n",
    "   \n",
    "    folder_path = os.path.join(labeled_data_path, item)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        if item == csv1.iloc[3][\"Unnamed: 1\"]:\n",
    "            index = change_name(csv1, folder_path, dossier_images_destination,1,idx)\n",
    "            idx = index\n",
    "        if item == csv2.iloc[3][\"Unnamed: 1\"]:    \n",
    "            index2 = change_name(csv2, folder_path, dossier_images_destination,2,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge les nouveaux csv\n",
    "csv = pd.concat([csv1, csv2], ignore_index=True)\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_bbox(clubhead, clubmiddle, clubbottom, pixel_box):\n",
    "\n",
    "    if (not np.isnan(clubhead[0]) and not np.isnan(clubhead[1])) and (not np.isnan(clubmiddle[0]) and not np.isnan(clubmiddle[1])) and (not np.isnan(clubbottom[0]) and not np.isnan(clubbottom[1])):\n",
    "        \n",
    "        bbox_orig = [min(clubhead[0], clubmiddle[0], clubbottom[0]),\n",
    "                min(clubhead[1], clubmiddle[1], clubbottom[1]),\n",
    "                max(clubhead[0], clubmiddle[0], clubbottom[0]),\n",
    "                max(clubhead[1], clubmiddle[1], clubbottom[1])]\n",
    "        \n",
    "        width = round(bbox_orig[2] - bbox_orig[0])\n",
    "        height = round(bbox_orig[3] - bbox_orig[1])\n",
    "\n",
    "        bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]\n",
    "        \n",
    "    elif (not np.isnan(clubhead[0]) and not np.isnan(clubhead[1])) and (not np.isnan(clubmiddle[0]) and not np.isnan(clubmiddle[1])):\n",
    "\n",
    "        bbox_orig = [min(clubhead[0], clubmiddle[0]),\n",
    "                min(clubhead[1], clubmiddle[1]),\n",
    "                max(clubhead[0], clubmiddle[0]),\n",
    "                max(clubhead[1], clubmiddle[1])]\n",
    "        \n",
    "        width = round(bbox_orig[2] - bbox_orig[0])\n",
    "        height = round(bbox_orig[3] - bbox_orig[1])\n",
    "\n",
    "        bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]\n",
    "            \n",
    "    elif (not np.isnan(clubmiddle[0]) and not np.isnan(clubmiddle[1])) and (not np.isnan(clubbottom[0]) and not np.isnan(clubbottom[1])):\n",
    "\n",
    "        bbox_orig = [min(clubmiddle[0], clubbottom[0]),\n",
    "                min(clubmiddle[1], clubbottom[1]),\n",
    "                max(clubmiddle[0], clubbottom[0]),\n",
    "                max(clubmiddle[1], clubbottom[1])]\n",
    "\n",
    "        width = round(bbox_orig[2] - bbox_orig[0])\n",
    "        height = round(bbox_orig[3] - bbox_orig[1])\n",
    "\n",
    "        bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)] \n",
    "               \n",
    "    elif (not np.isnan(clubhead[0]) and not np.isnan(clubhead[1])) and (not np.isnan(clubbottom[0]) and not np.isnan(clubbottom[1])):\n",
    "\n",
    "        bbox_orig = [min(clubhead[0], clubbottom[0]),\n",
    "                min(clubhead[1], clubbottom[1]),\n",
    "                max(clubhead[0], clubbottom[0]),\n",
    "                max(clubhead[1], clubbottom[1])]\n",
    "\n",
    "        width = round(bbox_orig[2] - bbox_orig[0])\n",
    "        height = round(bbox_orig[3] - bbox_orig[1])\n",
    "\n",
    "        bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]\n",
    "    else:\n",
    "\n",
    "        if (not np.isnan(clubhead[0]) and not np.isnan(clubhead[1])):\n",
    "\n",
    "            bbox_orig = [clubhead[0] - pixel_box, clubhead[1] - pixel_box, clubhead[0] + pixel_box, clubhead[1] + pixel_box]\n",
    "\n",
    "            width = round(bbox_orig[2] - bbox_orig[0])\n",
    "            height = round(bbox_orig[3] - bbox_orig[1])\n",
    "            bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]\n",
    "        \n",
    "        elif (not np.isnan(clubmiddle[0]) and not np.isnan(clubmiddle[1])):\n",
    "\n",
    "            bbox_orig = [clubmiddle[0] - pixel_box, clubmiddle[1] - pixel_box, clubmiddle[0] + pixel_box, clubmiddle[1] + pixel_box]\n",
    "\n",
    "            width = round(bbox_orig[2] - bbox_orig[0])\n",
    "            height = round(bbox_orig[3] - bbox_orig[1])\n",
    "            bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]\n",
    "            \n",
    "        elif (not np.isnan(clubbottom[0]) and not np.isnan(clubbottom[1])):\n",
    "\n",
    "            bbox_orig = [clubbottom[0] - pixel_box, clubbottom[1] - pixel_box, clubbottom[0] + pixel_box, clubbottom[1] + pixel_box]\n",
    "\n",
    "            width = round(bbox_orig[2] - bbox_orig[0])\n",
    "            height = round(bbox_orig[3] - bbox_orig[1])\n",
    "            bbox = [max(round(bbox_orig[0] - pixel_box),0), \n",
    "                max(round(bbox_orig[1] - pixel_box),0), \n",
    "                max(round(width + 2*pixel_box),0), \n",
    "                max(round(height + 2*pixel_box),0)]  \n",
    "            \n",
    "        else :\n",
    "            bbox = [0,0,0,0]\n",
    "            \n",
    "    return bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplissable des données dans le nouveau json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_annotations_images(dataframe,index_keypoint,index_image):\n",
    "    annotations = []\n",
    "    images = []\n",
    "    index_key = index_keypoint\n",
    "    index_image = index_image\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "\n",
    "        ################################### Fill images part \n",
    "        image_name = row['Unnamed: 2']\n",
    "        image_id = index  +index_image\n",
    "        #image_id = index - 3 +index_image\n",
    "        image_path = os.path.join(r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\\images\",image_name)\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "\n",
    "        image = {}\n",
    "        image = {\"id\": image_id,\"file_name\" : image_name, \"width\":width, \"height\":height, \"url\": image_path,\"group\" : 0}\n",
    "        \n",
    "        images.append(image)\n",
    "\n",
    "        ################################### Fill annotations part \n",
    "        keypoints_ranges = [(3, 8), (9, 14), (15, 20)]\n",
    "\n",
    "        for start, end in keypoints_ranges:\n",
    "\n",
    "            keypoints = []\n",
    "            annotation = {}\n",
    "\n",
    "            annotation = {\"id\": index_key, \"image_id\": image_id, \"category_id\": 1}\n",
    "\n",
    "            for j in range(start, end, 2):\n",
    "                if  pd.notna(row.iloc[j]) and pd.notna(row.iloc[j + 1]):\n",
    "                    x_value = float(row.iloc[j])\n",
    "                    y_value = float(row.iloc[j + 1])\n",
    "                    keypoints.extend([x_value, y_value, 2])  # Assuming all keypoints are labeled\n",
    "                else:\n",
    "                    keypoints.extend([np.nan, np.nan, np.nan])\n",
    "\n",
    "            clubhead = (keypoints[0],keypoints[1])\n",
    "            clubmiddle = (keypoints[3],keypoints[4])\n",
    "            clubbottom = (keypoints[6],keypoints[7])\n",
    "\n",
    "            if clubhead ==(np.nan,np.nan) and clubmiddle ==(np.nan,np.nan) and clubbottom ==(np.nan,np.nan):\n",
    "                pass \n",
    "            else :\n",
    "                annotation[\"keypoints\"] = keypoints\n",
    "                \n",
    "                bbox = form_bbox(clubhead, clubmiddle, clubbottom, 7) ## rajoute 10 pixels sur les cotés\n",
    "\n",
    "                annotation[\"bbox\"] = bbox\n",
    "\n",
    "                if sum(1 for k in range(len(keypoints)) if (keypoints[k] and keypoints[k] != 2)) == 2 or sum(1 for k in range(len(keypoints)) if (keypoints[k] and keypoints[k] != 2)) == 0 :\n",
    "                    annotation[\"iscrowd\"]=0\n",
    "                else:\n",
    "                    annotation[\"iscrowd\"]=1\n",
    "\n",
    "                annotation[\"num_keypoints\"] = sum(1 for k in range(len(keypoints)) if (keypoints[k] and keypoints[k] != 2))\n",
    "\n",
    "                annotations.append(annotation)\n",
    "\n",
    "                index_key +=1\n",
    "\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keypoint = 0\n",
    "index_image = 0 \n",
    "\n",
    "images,annotations = fill_annotations_images(csv,index_keypoint,index_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [\n",
    "    {\n",
    "        \"description\": \"Critac Project Dataset\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": 2024,\n",
    "        \"contributor\": \"Sebastien\",\n",
    "        \"date_created\": \"2024/02/27\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    {\n",
    "        \"supercategory\": \"Object\",\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Club\",\n",
    "        \"keypoints\": [\n",
    "            \"Clubhead\",\"Clubmiddle\",\"Clubbottom\"\n",
    "        ],\n",
    "        \"skeleton\": [\n",
    "            [0,1],[1,2],[0,2]\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"info\": info[0],\n",
    "    \"categories\": categories, \n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "}\n",
    "\n",
    "output_file = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\\annotations\"\n",
    "json_file_path = os.path.join(output_file,'annotations.json')\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    os.makedirs(output_file)\n",
    "\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de split je modifie le fichier annotations.json pour ajouter l'attribut group correspond si les keypoints sont proches et forme un group. group : 0 signifie aue les club sont bien espacé. group : 1 siginifie qu'ils sont proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melange et crée les 3 fichiers json train, valid et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(json_path,output,pourcent_train,pourcent_val):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    random.shuffle(data[\"images\"])\n",
    "\n",
    "    total_images = len(data[\"images\"])\n",
    "\n",
    "    group1_images = [img for img in data[\"images\"] if img[\"group\"] == 1]\n",
    "    group1_count = len(group1_images)\n",
    "    print(\"Nombre de frame avec l'attribut group à 1 :\", group1_count)\n",
    "\n",
    "    group0_images = [img for img in data[\"images\"] if img[\"group\"] == 0]\n",
    "    group0_count = len(group0_images)\n",
    "    print(\"Nombre de frame avec l'attribut group à 0 :\", group0_count)\n",
    "\n",
    "    group1_train_size = int(group1_count * pourcent_train)\n",
    "    group1_valid_size = int(group1_count * pourcent_val)\n",
    "\n",
    "    train_size = int(total_images * pourcent_train)\n",
    "    valid_size = int(total_images * pourcent_val)\n",
    "\n",
    "    #train_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": data[\"images\"][:train_size], \"annotations\": []}\n",
    "    #valid_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": data[\"images\"][train_size:train_size+valid_size], \"annotations\": []}\n",
    "    #test_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": data[\"images\"][train_size+valid_size:], \"annotations\": []}\n",
    "\n",
    "    train_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": [], \"annotations\": []}\n",
    "    valid_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": [], \"annotations\": []}\n",
    "    test_data = {\"categories\": data[\"categories\"], \"info\": data[\"info\"], \"images\": [], \"annotations\": []}\n",
    "\n",
    "    # Ajouter les images group1 aux données de train et de validation\n",
    "    train_data[\"images\"].extend(group1_images[:group1_train_size])\n",
    "    valid_data[\"images\"].extend(group1_images[group1_train_size:group1_train_size + group1_valid_size])\n",
    "    test_data[\"images\"].extend(group1_images[group1_train_size + group1_valid_size:])\n",
    "\n",
    "    # Ajouter le reste des images aux données de train, validation et test\n",
    "    non_group1_images = [img for img in data[\"images\"] if img[\"group\"] != 1]\n",
    "    train_data[\"images\"].extend(non_group1_images[:train_size - group1_train_size])\n",
    "    valid_data[\"images\"].extend(non_group1_images[train_size - group1_train_size:train_size + valid_size - group1_train_size - group1_valid_size])\n",
    "    test_data[\"images\"].extend(non_group1_images[train_size + valid_size - group1_train_size - group1_valid_size:])\n",
    "\n",
    "    group1_train = [img for img in train_data[\"images\"] if img[\"group\"] == 1]\n",
    "    group1_valid = [img for img in valid_data[\"images\"] if img[\"group\"] == 1]\n",
    "    group1_test = [img for img in test_data[\"images\"] if img[\"group\"] == 1]\n",
    "    group0_train = [img for img in train_data[\"images\"] if img[\"group\"] == 0]\n",
    "    group0_valid = [img for img in valid_data[\"images\"] if img[\"group\"] == 0]\n",
    "    group0_test = [img for img in test_data[\"images\"] if img[\"group\"] == 0]\n",
    "\n",
    "    print(\"Nombre de frame avec l'attribut group à 1 dans le split train :\", len(group1_train))\n",
    "    print(\"Nombre de frame avec l'attribut group à 1 dans le split valid :\", len(group1_valid))\n",
    "    print(\"Nombre de frame avec l'attribut group à 1 dans le split test :\", len(group1_test))\n",
    "    print(\"Nombre de frame avec l'attribut group à 0 dans le split train :\", len(group0_train))\n",
    "    print(\"Nombre de frame avec l'attribut group à 0 dans le split valid :\", len(group0_valid))\n",
    "    print(\"Nombre de frame avec l'attribut group à 0 dans le split test :\", len(group0_test))\n",
    "\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        if annotation[\"image_id\"] in [image[\"id\"] for image in train_data[\"images\"]]:\n",
    "            train_data[\"annotations\"].append(annotation)\n",
    "        elif annotation[\"image_id\"] in [image[\"id\"] for image in valid_data[\"images\"]]:\n",
    "            valid_data[\"annotations\"].append(annotation)\n",
    "        else:\n",
    "            test_data[\"annotations\"].append(annotation)\n",
    "\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "\n",
    "    with open(os.path.join(output, \"train.json\"), \"w\") as train_json_file:\n",
    "        json.dump(train_data, train_json_file, indent=4)\n",
    "\n",
    "    with open(os.path.join(output, \"valid.json\"), \"w\") as valid_json_file:\n",
    "        json.dump(valid_data, valid_json_file, indent=4)\n",
    "\n",
    "    with open(os.path.join(output, \"test.json\"), \"w\") as test_json_file:\n",
    "        json.dump(test_data, test_json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\\annotations\\annotations.json\"\n",
    "output = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\\annotations\"\n",
    "pourcent_train = 0.7\n",
    "pourcent_val = 0.15\n",
    "\n",
    "split(json_file,output,pourcent_train,pourcent_val)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\Dataset4\\annotations\\test.json\"\n",
    "data_path = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\Dataset4\\images\"\n",
    "\n",
    "# https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#loading-datasets-from-disk\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 700\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=0.5),  # Random change of brightness & contrast\n",
    "            A.HorizontalFlip(p=0.5),  # Randomly flips the input horizontally\n",
    "            A.VerticalFlip(p=0.5),  # Randomly flips the input vertically\n",
    "            A.CenterCrop (crop_size, crop_size, always_apply=False, p=1.0),\n",
    "            A.GaussNoise(var_limit=(10, 50), mean=0, p=0.5),\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),  # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='coco', label_fields=['bboxes_labels'])  # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, jsonfile, transform=None, demo=False):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.annotations_file = os.path.join(root, \"annotations\", jsonfile)  # Chemin vers le fichier JSON des annotations\n",
    "\n",
    "        with open(self.annotations_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.imgs_files = [images[\"file_name\"] for images in self.annotations[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        for image_info in self.annotations[\"images\"]:\n",
    "            if image_info[\"file_name\"] == self.imgs_files[idx]:\n",
    "                image_id = image_info[\"id\"]\n",
    "                break\n",
    "\n",
    "        annotations = []\n",
    "        for annotation_info in self.annotations[\"annotations\"]:\n",
    "            if annotation_info[\"image_id\"] == image_id:\n",
    "                annotations.append(annotation_info)\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ### taille des images\n",
    "        img_height, img_width = img_original.shape[:2]\n",
    "\n",
    "        ### Recupere les boites dans le format coco x,y,w,h et les keypoints\n",
    "        bboxes_original = []\n",
    "        keypoints_originals = []\n",
    "        iscrowd = []\n",
    "        for annotation in annotations:\n",
    "\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            bboxes_original.append(bbox)\n",
    "            iscrowd.append(annotation[\"iscrowd\"])\n",
    "\n",
    "            kp = annotation[\"keypoints\"]\n",
    "            keypoints_originals.append(kp)\n",
    "\n",
    "        #### Crée une liste avec les keypoints dans le format coco pour la transformation\n",
    "        #### et crée une liste dans le format x1, y1, x2, y2 pour la visualisation\n",
    "        bboxes_original_xywh = []\n",
    "        bboxes_original_xy = []\n",
    "        for bbox in bboxes_original:\n",
    "            x1, y1, w, h = bbox\n",
    "            bboxes_original_xywh.append([x1,y1,w,h])\n",
    "            bboxes_original_xy.append([min(max(x1,0),img_width), min(max(y1,0),img_height), min(max(w+x1,0),img_width), min(max(h+y1,0),img_height)])\n",
    "\n",
    "        #### Convertis le format coco des annotations keypoints [x,y,visibility, x,y,visibility, x,y,visibility]\n",
    "        ####  au format [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "        #### Une liste avec les NaN, ainsi que leurs indices et une liste sans les NaN pour la transformations\n",
    "        keypoints_original = []\n",
    "        keypoints_original_with_zero = []\n",
    "        zero_indices = []\n",
    "        for i in range(len(keypoints_originals)):\n",
    "            keypoints = keypoints_originals[i]\n",
    "            keypoints_list = []\n",
    "            keypoints_list_with_zero = []\n",
    "            for j in range(0, len(keypoints), 3):\n",
    "                if keypoints[j:j+3] == [0.5, 0.5, 0]:\n",
    "                    zero_indices.append([i, j//3])\n",
    "                    keypoints_list_with_zero.append([0.5, 0.5, 0])\n",
    "                else:\n",
    "                    keypoints_list.append([keypoints[j], keypoints[j+1], keypoints[j+2]])\n",
    "                    keypoints_list_with_zero.append([keypoints[j], keypoints[j+1], keypoints[j+2]])\n",
    "            keypoints_original.append(keypoints_list)\n",
    "            keypoints_original_with_zero.append(keypoints_list_with_zero)\n",
    "\n",
    "        # All objects are glue tubes\n",
    "        bboxes_labels_original = ['Club' for _ in bboxes_original_xy]\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "\n",
    "            keypoints_original_flattened = []\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original_xywh, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "            #### Mets le format de la bounding box x,y,x,y\n",
    "            bboxes_xy = []\n",
    "            for bbox in bboxes:\n",
    "                x1, y1, w, h = bbox\n",
    "                x1 = round(max(min(x1, img_width), 0), 2)\n",
    "                y1 = round(max(min(y1, img_height), 0), 2)\n",
    "                x2 = round(max(min(x1 + w, img_width), 0), 2)\n",
    "                y2 = round(max(min(y1 + h, img_height), 0), 2)\n",
    "                bboxes_xy.append([x1, y1, x2, y2])\n",
    "\n",
    "            keypoints_transformed = transformed['keypoints']\n",
    "\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            # Divertly Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "\n",
    "            #### Remplis avec les NaN et les keypoints dans les listes\n",
    "            keypoint = []\n",
    "            keypoint_idx = 0\n",
    "            for i in range(len(keypoints_original_with_zero)):\n",
    "                obj_keypoints = []\n",
    "                for j in range(len(keypoints_original_with_zero[i])):\n",
    "                    if [i, j] in zero_indices:  # Vérifie si cet indice était un NaN\n",
    "                        obj_keypoints.append(0.5)\n",
    "                        obj_keypoints.append(0.5)\n",
    "                        obj_keypoints.append(0)\n",
    "                    else:\n",
    "                        if len(keypoints_transformed) <= keypoint_idx:\n",
    "                          obj_keypoints.append(0.5)\n",
    "                          obj_keypoints.append(0.5)\n",
    "                          obj_keypoints.append(0)\n",
    "                        else :\n",
    "                            obj_keypoints.extend(keypoints_transformed[keypoint_idx])\n",
    "                            obj_keypoints.append(2)\n",
    "                        keypoint_idx += 1\n",
    "\n",
    "                keypoint.append(obj_keypoints)\n",
    "\n",
    "            #### Reconstruis la liste de liste\n",
    "            keypoints = []\n",
    "            for i in range(len(keypoint)):\n",
    "                obj_keypoint = []\n",
    "                for j in range(0,len(keypoint[i]),3):\n",
    "                    obj_keypoint.append([keypoint[i][j], keypoint[i][j+1], keypoint[i][j+2]])\n",
    "                keypoints.append(obj_keypoint)\n",
    "\n",
    "        else:\n",
    "            img, bboxes_xy, keypoints = img_original, bboxes_original_xy, keypoints_original_with_zero\n",
    "\n",
    "        # Convert everything into a torch tensor\n",
    "        bboxes = torch.as_tensor(bboxes_xy, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n",
    "        #target[\"image_id\"] = torch.tensor([image_id])\n",
    "        target[\"image_id\"] = image_id\n",
    "        if len(bboxes.shape) == 1:\n",
    "            # bboxes est unidimensionnel, donc on le convertit en bidimensionnel\n",
    "            bboxes = bboxes.unsqueeze(0)\n",
    "            # on vérifie si bboxes est vide\n",
    "            if bboxes.shape[1] == 0:\n",
    "                # bboxes est vide, donc on définit target[\"area\"] comme un tenseur vide\n",
    "                target[\"area\"] = torch.tensor([], dtype=torch.float32)\n",
    "            else:\n",
    "                # on calcule l'aire en utilisant la formule appropriée pour un tenseur unidimensionnel\n",
    "                target[\"area\"] = (bboxes[0, 2] - bboxes[0, 0]) * (bboxes[0, 3] - bboxes[0, 1])\n",
    "        else:\n",
    "            # bboxes est bidimensionnel, donc on calcule l'aire en utilisant la formule appropriée pour un tenseur bidimensionnel\n",
    "            target[\"area\"] = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original_xy, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        #target_original[\"image_id\"] = torch.tensor([image_id])\n",
    "        target_original[\"image_id\"] = image_id\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original_with_zero, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\dataset_exp\"\n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, \"train.json\", transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: 'CH', 1: 'CM', 2:'CB'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 5\n",
    "\n",
    "    img_width, img_height = image.shape[:2]\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    idx_nan =[]\n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 5, (255,0,0), 4)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "            if kp == [np.nan,np.nan] :\n",
    "                idx_nan = idx_nan.append(idx)\n",
    "        if idx_nan == 0:\n",
    "            image = cv2.line(image.copy(), kps[1], kps[2], (0,0,0), 1) \n",
    "        elif idx_nan == 1:\n",
    "            image = cv2.line(image.copy(), kps[0], kps[2], (0,0,0), 1) \n",
    "        elif idx_nan == 2:\n",
    "            image = cv2.line(image.copy(), kps[0], kps[1], (0,0,0), 1) \n",
    "        elif idx_nan == []:\n",
    "            image = cv2.line(image.copy(), kps[0], kps[1], (0,0,0), 1) \n",
    "            image = cv2.line(image.copy(), kps[1], kps[2], (0,0,0), 1) \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "        \n",
    "        idx_nan =[]\n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 2)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "                \n",
    "                if kp == [np.nan,np.nan] :\n",
    "                    idx_nan = idx_nan.append(idx)\n",
    "            if idx_nan == 0:\n",
    "                image = cv2.line(image.copy(), kps[1], kps[2], (0,0,0), 1) \n",
    "            elif idx_nan == 1:\n",
    "                image = cv2.line(image.copy(), kps[0], kps[2], (0,0,0), 1) \n",
    "            elif idx_nan == 2:\n",
    "                image = cv2.line(image.copy(), kps[0], kps[1], (0,0,0), 1) \n",
    "            elif idx_nan == []:\n",
    "                image = cv2.line(image.copy(), kps[0], kps[1], (0,0,0), 1) \n",
    "                image = cv2.line(image.copy(), kps[1], kps[2], (0,0,0), 1) \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des données augmentées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0)) # object with specified anchor sizes and aspect ratios.\n",
    "    \n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, # he detection head (the part responsible for object detection and keypoint prediction) will be initialized with random weights\n",
    "                                                                   pretrained_backbone=True, #backbone network (ResNet-50) will be initialized with pre-trained weights from ImageNet\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   trainable_backbone_layers = 5,\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\New_Dataset\"\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER,\"train.json\", transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER, \"valid.json\", transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER,\"test.json\", transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(num_keypoints = 3)\n",
    "model.to(device)\n",
    "model_without_ddp = model\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001, betas = (0.9, 0.999),eps = 1e-8, weight_decay=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.6)\n",
    "# step_size (int): Period of learning rate decay.\n",
    "# gamma (float): Multiplicative factor of learning rate decay. Default: 0.1.\n",
    "\n",
    "start_epoch = 0\n",
    "num_epochs = 20\n",
    "\n",
    "amp = False #Automatic Mixed Precision, une fonctionnalité fournie par la bibliothèque PyTorch \n",
    "           #pour accélérer l'entraînement des réseaux de neurones en utilisant un mélange de précision simple \n",
    "           #(float16) et double (float32) pour les calculs.\n",
    "\n",
    "output_dir = r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\Experiences\\Exp10\"\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "\n",
    "csv_path = os.path.join(output_dir,'loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger un checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path= r\"C:\\Users\\mcossin\\Documents\\object_detection\\code_detection\\keypoint_detection\\Experiences\\Exp10\\model_19_baseline.pth\"\n",
    "\n",
    "model_to_retrain = os.path.join(output_dir,'model_19_baseline.pth')\n",
    "model = get_model(num_keypoints = 3, weights_path=model_to_retrain)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    metric_logger = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1,csv_path = csv_path, scaler = scaler)\n",
    "    lr_scheduler.step()\n",
    "    #enregistre toutes les 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, f\"model_{epoch}_baseline.pth\"))\n",
    "\n",
    "\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, data_loader_val, device=device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.read_csv(csv_path,header = None)\n",
    "\n",
    "loss = loss[0].str[7:13]\n",
    "loss= loss.astype(float)\n",
    "\n",
    "num_iters_per_epoch = len(dataset_train) // batch_size\n",
    "\n",
    "loss_epoch = [l for i, l in enumerate(loss) if i % (num_iters_per_epoch + 1) == 0]\n",
    "\n",
    "epoch_list = range(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss.index, loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, loss_epoch[2:])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Loss Curve by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-8, 1e-2)\n",
    "    betas = (0.9, 0.999) #(trial.suggest_uniform('beta1', 0.5, 0.999), trial.suggest_uniform('beta2', 0.9, 0.999))\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "\n",
    "    # Your model and data loaders here\n",
    "    model = get_model(num_keypoints = 3)\n",
    "    model.to(device)\n",
    "    model_without_ddp = model\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=betas, eps=1e-8, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=gamma)\n",
    "\n",
    "    # Your training and evaluation functions here\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1, scaler = scaler)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": model_without_ddp.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        \n",
    "        utils.save_on_master(checkpoint, os.path.join(output_dir, f\"model_epoch{epoch}_lr{lr}_weight_decay{weight_decay}_gamma{gamma}.pth\"))\n",
    "        utils.save_on_master(checkpoint, os.path.join(output_dir, f\"checkpoint_epoch{epoch}_lr{lr}_weight_decay{weight_decay}_gamma{gamma}.pth\"))\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        coco_evaluator = evaluate(model, data_loader_val, device=device)\n",
    "        mAP = coco_evaluator.coco_eval[\"keypoints\"].stats[0]\n",
    "        mAR = coco_evaluator.coco_eval[\"keypoints\"].stats[5]\n",
    "        mean_F1 = 2 * (mAP * mAR) / (mAP + mAR)\n",
    "\n",
    "    return mean_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "study = optuna.create_study(direction='maximise')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print('Best hyperparameters: ', best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate after training and validation are finished\n",
    "\n",
    "model_to_test = os.path.join(output_dir,'model_9_baseline.pth')\n",
    "model = get_model(num_keypoints = 3, weights_path=model_to_test)\n",
    "print(\"\\nStart testing\")\n",
    "coco_evaluator = evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "keypoints_eval_info = coco_evaluator.coco_eval['keypoints'].stats.tolist()\n",
    "print(\"Validation Keypoints mAP progression: \", keypoints_eval_info[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle entraîné\n",
    "model_to_test = os.path.join(output_dir,'model_19_baseline.pth')\n",
    "model = get_model(num_keypoints = 3, weights_path=model_to_test)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transférer le modèle sur l'appareil cible\n",
    "model = model.to(device)\n",
    "\n",
    "# Définir le modèle en mode d'évaluation\n",
    "model.eval()\n",
    "\n",
    "# Définir le chemin d'accès à la vidéo\n",
    "video_path = r\"C:\\Users\\mcossin\\Documents\\object_detection\\Data\\youtube_video\\test\\video-youtube-1_test.mp4\"\n",
    "\n",
    "# Ouvrir la vidéo avec OpenCV\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Lire la largeur et la hauteur de la vidéo\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Définir le codec et le fichier de sortie pour enregistrer la vidéo avec les prédictions\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_file = os.path.join(r\"C:\\Users\\mcossin\\Documents\\object_detection\\Data\\youtube_video\\video_inference\",'video.mp4')\n",
    "out = cv2.VideoWriter(out_file, fourcc, 20.0, (width, height))\n",
    "\n",
    "fc = 0\n",
    "fps = 0\n",
    "tfc = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "tfcc = 0\n",
    "\n",
    "print(f\"Total frame count : {tfc}\")\n",
    "\n",
    "# Lire la vidéo image par image\n",
    "while tfcc < tfc:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Lire une image de la vidéo\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Si la fin de la vidéo est atteinte, sortir de la boucle\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir l'image en tenseur PyTorch\n",
    "    image = torch.from_numpy(frame).permute(2, 0, 1).float().unsqueeze(0) / 255\n",
    "    #image = (frame.permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Effectuer une prédiction avec le modèle\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Extraire les prédictions de la sortie du modèle\n",
    "    scores = output[0]['scores'].detach().cpu().numpy()\n",
    "    high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "    post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "    keypoints = []\n",
    "    for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "    bboxes = []\n",
    "    for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "    # Visualiser les prédictions sur l'image\n",
    "    frame = visualize(frame, bboxes, keypoints)\n",
    "\n",
    "    # Écrire l'image avec les prédictions dans le fichier de sortie\n",
    "    out.write(frame)\n",
    "\n",
    "    # Calculer le FPS\n",
    "    fc += 1\n",
    "    end_time = time.time()\n",
    "    fps += 1/np.round(end_time - start_time, 3)\n",
    "    if fc == 10:\n",
    "        fps = int(fps / 10)\n",
    "        tfcc += fc\n",
    "        fc = 0\n",
    "        per_com = int(tfcc / tfc * 100)\n",
    "        print(f\"Frames Per Second : {fps} || Percentage Parsed : {per_com}\")\n",
    "        start_time = end_time\n",
    "\n",
    "# Libérer les ressources OpenCV\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)\n",
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toutes les images test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(data_loader_test):\n",
    "    images = list(image.to(device) for image in images)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "\n",
    "    for j, image in enumerate(images):\n",
    "        image = (image.permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        scores = output[j]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[j]['boxes'][high_scores_idxs], output[j]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[j]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        bboxes = []\n",
    "        for bbox in output[j]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "\n",
    "        print(f\"Predictions for Batch {i + 1}, Image {j + 1}:\")\n",
    "        print(\"Bounding boxes:\", bboxes)\n",
    "        print(\"Keypoints:\", keypoints)\n",
    "\n",
    "        visualize(image, bboxes, keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
